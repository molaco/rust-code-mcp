performance_comparison:
  project: rust-code-mcp vs claude-context
  research_date: 2025-10-19
  
  # ============================================================================
  # RUST-CODE-MCP PERFORMANCE PROFILE
  # ============================================================================
  
  rust_code_mcp:
    
    primary_claims:
      codebase_size_target: "1M+ LOC"
      query_latency_target: "<200ms p95"
      status: "Target (not benchmarked in production)"
      incremental_indexing: "10x+ faster than full reindex"
      token_reduction_projection: "45-50% (vs grep)"
      
    scalability_targets:
      mvp_week_10:
        codebase_size: "100k LOC"
        index_time: "<2 minutes"
        query_latency_p95: "<500ms"
        memory_usage: "<2GB"
        status: "Target specification"
        
      production_week_16:
        codebase_size: "1M LOC"
        index_time: "<5 minutes"
        query_latency_p95: "<200ms"
        memory_usage: "<4GB"
        retrieval_accuracy: ">80% NDCG"
        status: "Target specification"
        
    indexing_performance:
      detailed_targets:
        - size: "10k LOC"
          initial_index: "<30 sec"
          incremental_update: "<1 sec"
          search_latency: "<100ms"
          
        - size: "100k LOC"
          initial_index: "<2 min"
          incremental_update: "<2 sec"
          search_latency: "<150ms"
          
        - size: "500k LOC"
          initial_index: "<5 min"
          incremental_update: "<3 sec"
          search_latency: "<200ms"
          
        - size: "1M LOC"
          initial_index: "<10 min"
          incremental_update: "<5 sec"
          search_latency: "<300ms"
          
        - size: "10M LOC"
          initial_index: "<1 hour"
          incremental_update: "<2 min"
          search_latency: "<100ms (with Merkle)"
          note: "Extended target with optimization"
          
      actual_measurements:
        - test: "Fresh indexing 3 files (368 LOC)"
          result: "~50ms"
          status: "Working ✓"
          source: "PHASE1_PLAN.md"
          
        - test: "Incremental: second search (no changes)"
          result: "<10ms"
          speedup: "10x+ faster"
          status: "Working ✓"
          source: "README.md"
          
        - test: "Incremental: 1 file change"
          result: "~15-20ms"
          note: "Only reindexes changed file"
          status: "Working ✓"
          
      incremental_optimization:
        change_detection_method: "SHA-256 (current), Merkle tree (planned)"
        unchanged_check_latency:
          with_sha256: "Linear scan of files"
          with_merkle_10k_loc: "<10ms"
          with_merkle_100k_loc: "<20ms"
          with_merkle_1m_loc: "<50ms"
          with_merkle_10m_loc: "<100ms"
          improvement: "100x faster for large codebases"
          
    query_latency:
      current_vector_only:
        embedding_generation: "5-20ms"
        vector_search_qdrant: "<50ms (10 results)"
        total: "<100ms"
        status: "Designed, awaiting Qdrant population"
        source: "PHASE6_COMPLETE.md"
        
      planned_hybrid_search:
        embedding_generation: "5-20ms"
        vector_search: "<50ms"
        bm25_search: "<20ms (Tantivy)"
        rrf_merging: "<5ms"
        total: "<100ms (parallel execution)"
        status: "Infrastructure ready, Qdrant pipeline missing"
        
      tool_specific_latencies:
        search: "<100ms (with persistent index)"
        read_file_content: "<10ms"
        find_definition: "100-500ms (depends on size)"
        find_references: "100-500ms (depends on size)"
        get_dependencies: "<50ms"
        get_call_graph: "<50ms"
        analyze_complexity: "<100ms"
        get_similar_code: "200-1000ms (network + embedding)"
        source: "TESTING.md"
        
    memory_usage:
      mvp_target: "<2GB (typical usage)"
      production_target: "<4GB (1M LOC)"
      
      vector_storage_config:
        embedding_dimensions: 384
        model: "all-MiniLM-L6-v2"
        model_size: "80MB"
        hnsw_m: 16
        hnsw_ef_construct: 100
        memmap_threshold: "50,000 vectors"
        note: "Memory-mapped after 50k vectors for efficiency"
        
      merkle_tree_overhead:
        per_file: "~1-2 KB"
        metadata_cache: "~200 bytes per file"
        estimated_1m_loc: "50-100 MB"
        
      optimization_notes:
        - "GitHub Copilot embedding: 8x memory reduction vs baseline"
        - "Qdrant uses memory-mapped storage after 50k vectors"
        
    maximum_supported_codebase:
      explicit_target: "1M+ LOC"
      design_goal: "Enterprise-scale Rust codebases"
      reference_projects: "rustc, tokio, serde"
      
      qdrant_scaling:
        excellent_for: "<10M LOC (single server)"
        deployment_mode: "Embedded (default)"
        scales_to: "50M+ vectors with optimization"
        
      industry_comparison:
        meta_glean: "100M+ LOC"
        augment_code: "100M+ LOC"
        rust_code_mcp_range: "1M-10M LOC (achievable)"
        
    retrieval_quality_targets:
      mvp:
        ndcg_at_10: ">0.65"
        mrr: ">0.70"
        recall_at_20: ">0.85"
        latency_p95: "<500ms"
        
      production:
        ndcg_at_10: ">0.75"
        mrr: ">0.80"
        recall_at_20: ">0.95"
        latency_p95: "<200ms"
        
      expected_improvements:
        token_reduction_vs_grep: "45-50%"
        token_reduction_vs_claude_context: "Better than 40%"
        reason: "BM25 + vector + RRF fusion"
        
    technology_stack_performance:
      embedding_model:
        name: "all-MiniLM-L6-v2"
        dimensions: 384
        speed: "14.7ms per 1K tokens"
        size: "80MB"
        quality: "★★★ (good baseline)"
        
      vector_database:
        name: "Qdrant"
        query_latency: "10-30ms (vs Milvus ~50ms)"
        insertion_speed: "41.27s (SQuAD dataset)"
        optimization: "<50M vectors single server"
        
      text_search:
        name: "Tantivy"
        expected_latency: "<20ms"
        note: "Rust-native, optimized for performance"
        
    implementation_status:
      working:
        - "Tantivy indexing with SHA-256 change detection"
        - "Incremental updates (10x+ faster verified)"
        - "Tree-sitter parsing"
        - "Code chunking with context"
        - "Hybrid search infrastructure (RRF ready)"
        - "8 MCP tools implemented"
        
      critical_gaps:
        - "Qdrant population pipeline (CRITICAL)"
        - "Background file watching (optional)"
        - "Merkle tree optimization (for >500k LOC)"
        - "Production deployment configuration"
        
      test_status:
        unit_tests: "45 passed; 0 failed; 11 ignored"
        integration_tests: "8 MCP tools verified"
        performance_benchmarks: "Not yet executed"
        
    documentation_sources:
      - "README.md - Primary claims"
      - "docs/NEW_PLAN.md - Detailed targets"
      - "docs/DEEP_RESEARCH_FINDINGS.md - Performance analysis"
      - "docs/INDEXING_STRATEGIES.md - Implementation strategies"
      - "docs/PHASE6_COMPLETE.md - Current measurements"
      - "TESTING.md - Validation plan"
      
  # ============================================================================
  # CLAUDE-CONTEXT PERFORMANCE PROFILE
  # ============================================================================
  
  claude_context:
    
    primary_claims:
      token_reduction: "40% (verified in production)"
      status: "Production-deployed, proven performance"
      codebase_support: "Millions of LOC"
      scalability: "Elastic with Zilliz Cloud backend"
      
    token_efficiency:
      verified_achievement: "40% reduction vs grep-only"
      condition: "Equivalent retrieval quality maintained"
      impact: "Significant cost and time savings"
      
      comparative_benchmarks:
        cursor_ide: "30-40% token reduction"
        optimization_engines: "Up to 76% in some implementations"
        
      task_specific_improvements:
        find_implementation:
          grep_time: "5 min (multi-round)"
          claude_context: "Instant"
          speedup: "300x faster"
          
        refactoring:
          grep_tokens: "High token cost"
          claude_context: "40% less tokens"
          efficiency: "1.67x efficient"
          
        bug_investigation:
          grep_searches: "Multiple searches"
          claude_context: "Single query"
          speedup: "3-5x faster"
          
    query_latency:
      zilliz_cloud_backend:
        p99_latency: "<50ms (concurrent loads)"
        production_requirement: "<300ms (3,500+ dimensions)"
        target_latency: "10-20ms (real-time applications)"
        
      milvus_benchmarks:
        version_2_2_3: "2.5x reduction vs 2.0.0"
        qps_improvement: "4.5x increase"
        
      qualitative_performance:
        description: "Immediate pinpointing of exact file/line"
        contrast: "vs five-minute grep-powered goose chase"
        note: "No specific millisecond measurements published"
        
      cardinal_vector_engine:
        query_throughput: "10x higher"
        index_building: "3x faster than open-source"
        production_testing: "Consistent sub-50ms p99"
        
    indexing_performance:
      initial_indexing:
        description: "A few minutes depending on codebase size"
        method: "Intelligent chunking + embedding generation"
        background: "Allows users to continue working"
        
      incremental_updates:
        change_detection: "Merkle tree (millisecond-level)"
        unchanged_detection: "O(1) via Merkle root comparison"
        reindex_policy: "Only changed files"
        snapshot_location: "~/.context/merkle/"
        
      merkle_tree_performance:
        build_tree_10k_files: "~100ms"
        root_hash_check: "<1ms"
        detect_changes: "10-50ms"
        update_single_file: "<1ms"
        
      data_insertion_benchmarks:
        milvus: "12.02s (SQuAD dataset)"
        qdrant: "41.27s (SQuAD dataset)"
        milvus_advantage: "3.4x faster for bulk indexing"
        
      projected_targets:
        - size: "10k LOC"
          first_index: "<30s"
          incremental_1pct: "<1s"
          unchanged_check: "<10ms"
          
        - size: "100k LOC"
          first_index: "<2min"
          incremental_1pct: "<3s"
          unchanged_check: "<20ms"
          
        - size: "500k LOC"
          first_index: "<5min"
          incremental_1pct: "<8s"
          unchanged_check: "<50ms"
          
        - size: "1M LOC"
          first_index: "<10min"
          incremental_1pct: "<15s"
          unchanged_check: "<100ms"
          
    memory_usage:
      published_metrics: "None specific to claude-context"
      
      related_benchmarks:
        tantivy_0_22: "22% less memory (590MB vs 760MB on HDFS)"
        merkle_per_file: "~1-2 KB"
        metadata_cache: "~200 bytes per file"
        total_overhead_1m_loc: "50-100 MB"
        
      qdrant_memory_management:
        hnsw_parameters: "Configurable"
        memory_mapped_files: "For large datasets"
        memmap_threshold: "Configurable"
        
    maximum_supported_codebase:
      official_claims:
        - "Millions of lines of code"
        - "No matter how large your codebase is"
        - "Elastic scaling with Zilliz Cloud"
        
      best_practices:
        recommendation: "Don't index massive monorepos all at once"
        approach: "Start with specific components/repositories"
        architecture: "Each path maintains own collection + Merkle tree"
        
      infrastructure_scalability:
        zilliz_cloud: "Enterprise-grade distributed deployment"
        vector_support: ">100M vectors"
        availability: "Multi-replica"
        note: "Cloud-native architecture"
        
    retrieval_quality:
      ast_based_chunking_benefits:
        starcoder2_7b_repeval: "+5.5 points average gain"
        crosscodeeval: "+4.3 points"
        swe_bench: "+2.7 points"
        contextual_retrieval: "+49% improvement"
        
      embedding_performance:
        fastembed_vs_ollama: "16x faster (3 hours vs 2+ days)"
        with_optimization: "2.2x faster (674s vs 1463s)"
        with_ray_distributed: "7.5x faster (195s)"
        
    architecture:
      vector_database: "Milvus / Zilliz Cloud (managed)"
      code_parsing: "tree-sitter (multi-language AST)"
      
      embedding_models:
        openai: "text-embedding-3-large (3072d)"
        voyage_ai: "voyage-code-3 (code-specialized)"
        ollama: "Local models (privacy-first)"
        
      search_method: "Hybrid (BM25 + dense vector)"
      change_detection: "Merkle trees"
      
      language_support:
        languages:
          - "JavaScript"
          - "Python"
          - "Java"
          - "Go"
        method: "tree-sitter parsers with AST-based semantic chunking"
        
    component_performance:
      qdrant_benchmarks:
        rps_gains: "4x on some datasets"
        p95_latency: "39% lower (36.73ms vs 60.42ms vs Postgres)"
        p99_latency: "48% better (38.71ms vs 74.60ms)"
        sparse_vector_v1_8: "16x faster throughput"
        
      tree_sitter_parsing:
        symflower_migration: "36x speedup vs JavaParser"
        haskell_parser: "48.2x - 52.8x speedup"
        memory: "Memory-efficient incremental parsing"
        
    production_status:
      deployment: "Production-deployed across multiple organizations"
      validation: "Proven 40% token reduction in real usage"
      maturity: "Cloud-native, battle-tested"
      
    limitations:
      unpublished_metrics:
        - "Absolute query response times (milliseconds)"
        - "Specific memory requirements per codebase size"
        - "Exact indexing speed (files/sec or lines/sec)"
        - "Maximum tested codebase size"
        - "Detailed Merkle tree operation timings"
        
      qualitative_vs_quantitative:
        note: "Most claims qualitative ('milliseconds', 'instant', 'fast')"
        hard_numbers: "Limited in official documentation"
        variability: "Performance varies by deployment configuration"
        
    documentation_sources:
      - "Web: github.com/zilliztech/claude-context"
      - "Local: docs/COMPARISON_CLAUDE_CONTEXT.md"
      - "Local: docs/DEEP_RESEARCH_FINDINGS.md"
      - "Local: docs/ADVANCED_RESEARCH.md"
      
  # ============================================================================
  # COMPARATIVE ANALYSIS
  # ============================================================================
  
  comparison:
    
    token_reduction:
      rust_code_mcp: "45-50% projected (not validated)"
      claude_context: "40% verified (production)"
      advantage: "rust-code-mcp (projected, pending validation)"
      reason: "True hybrid BM25+Vector vs vector-focused approach"
      
    query_latency:
      rust_code_mcp: 
        target: "<200ms p95 (production target)"
        current: "<100ms (hybrid, pending Qdrant pipeline)"
        status: "Designed but not benchmarked"
        
      claude_context:
        achieved: "<50ms p99 (Zilliz Cloud, concurrent loads)"
        requirement: "<300ms (production, 3,500+ dims)"
        status: "Production-verified"
        
      advantage: "claude-context (proven performance)"
      note: "rust-code-mcp has good potential but unproven"
      
    indexing_time:
      rust_code_mcp_1m_loc:
        initial: "<10 min (target)"
        incremental_1pct: "<5s (target)"
        unchanged_check: "<50ms (with Merkle)"
        actual_small: "~50ms (368 LOC, verified)"
        
      claude_context_1m_loc:
        initial: "<10 min (projected)"
        incremental_1pct: "<15s (projected)"
        unchanged_check: "<100ms (Merkle root comparison)"
        note: "Qualitative measurements only"
        
      advantage: "Similar targets, rust-code-mcp has better incremental specs"
      validation: "Neither has large-scale benchmarks published"
      
    memory_usage:
      rust_code_mcp:
        target: "<4GB for 1M LOC"
        embedding_model: "80MB (all-MiniLM-L6-v2)"
        merkle_overhead: "50-100 MB estimated"
        status: "Target specification"
        
      claude_context:
        published: "None specific"
        merkle_overhead: "50-100 MB estimated"
        status: "No published metrics"
        
      advantage: "rust-code-mcp (at least has targets)"
      note: "Both projects lack real-world memory profiling data"
      
    maximum_codebase_size:
      rust_code_mcp:
        explicit: "1M+ LOC"
        optimal_range: "1M-10M LOC"
        infrastructure: "Qdrant embedded (default), <50M vectors"
        
      claude_context:
        explicit: "Millions of LOC"
        infrastructure: "Zilliz Cloud (elastic, >100M vectors)"
        recommendation: "Don't index massive monorepos all at once"
        
      advantage: "claude-context (cloud-native scalability)"
      note: "Zilliz Cloud provides true elastic scaling"
      
    change_detection:
      rust_code_mcp:
        current: "SHA-256 (linear scan)"
        planned: "Merkle tree (for >500k LOC)"
        expected_improvement: "100x faster for large codebases"
        status: "Designed but not implemented"
        
      claude_context:
        implemented: "Merkle tree (production)"
        performance: "O(1) unchanged detection"
        timings: "<1ms root hash check, 10-50ms change detection"
        status: "Production-deployed"
        
      advantage: "claude-context (implemented and proven)"
      lesson: "Merkle trees critical for large-scale incremental updates"
      
    search_architecture:
      rust_code_mcp:
        approach: "True hybrid (Tantivy BM25 + Qdrant vector + RRF)"
        bm25_latency: "<20ms"
        vector_latency: "<50ms"
        total: "<100ms (parallel)"
        status: "Infrastructure ready, Qdrant pipeline missing"
        
      claude_context:
        approach: "Hybrid (BM25 + dense vector via Milvus)"
        method: "Combined lexical + semantic search"
        latency: "<50ms p99"
        status: "Production-deployed"
        
      advantage: "claude-context (working end-to-end)"
      potential: "rust-code-mcp has comparable design once completed"
      
    production_maturity:
      rust_code_mcp:
        status: "In development"
        validation: "Unit tests: 45 passed, integration tests: 8 tools"
        deployment: "Not production-deployed"
        benchmarks: "Limited to small-scale tests"
        
      claude_context:
        status: "Production-deployed"
        validation: "Real-world usage across organizations"
        deployment: "Cloud-native, battle-tested"
        benchmarks: "40% token reduction verified"
        
      advantage: "claude-context (proven in production)"
      note: "rust-code-mcp has solid foundation but needs validation"
      
    infrastructure_dependencies:
      rust_code_mcp:
        vector_db: "Qdrant (embedded or remote)"
        text_search: "Tantivy (embedded)"
        embedding: "FastEmbed (local model)"
        deployment: "Self-hosted by default"
        
      claude_context:
        vector_db: "Milvus / Zilliz Cloud (managed service)"
        text_search: "Integrated with Milvus"
        embedding: "OpenAI / Voyage AI / Ollama"
        deployment: "Cloud-native or self-hosted"
        
      advantage: "Depends on use case"
      rust_code_mcp_benefit: "Self-contained, no cloud dependencies"
      claude_context_benefit: "Enterprise scalability via cloud"
      
    technology_choices:
      embedding_models:
        rust_code_mcp: "all-MiniLM-L6-v2 (384d, 14.7ms/1K tokens)"
        claude_context: "Multiple options (OpenAI 3072d, Voyage, Ollama)"
        note: "claude-context offers more flexibility"
        
      vector_databases:
        rust_code_mcp: "Qdrant (10-30ms query, 41s insertion)"
        claude_context: "Milvus (50ms query, 12s insertion)"
        insertion_advantage: "Milvus 3.4x faster"
        query_advantage: "Qdrant 2x faster"
        
      text_search:
        rust_code_mcp: "Tantivy (Rust-native, <20ms expected)"
        claude_context: "Integrated BM25 in Milvus"
        note: "Tantivy v0.22: 22% less memory, optimized"
        
    critical_gaps:
      rust_code_mcp:
        - "Qdrant population pipeline (blocks hybrid search)"
        - "Large-scale benchmarks (no 100k+ LOC tests)"
        - "Merkle tree implementation (for efficiency at scale)"
        - "Production deployment configuration"
        - "Memory profiling on real codebases"
        
      claude_context:
        - "Published quantitative metrics (latency, throughput)"
        - "Maximum tested codebase size documentation"
        - "Memory usage specifications"
        - "Detailed performance breakdowns"
        
  # ============================================================================
  # KEY FINDINGS & RECOMMENDATIONS
  # ============================================================================
  
  key_findings:
    
    performance_validation_status:
      rust_code_mcp: "Strong design, limited validation"
      claude_context: "Production-proven, limited published metrics"
      
    scalability_approach:
      rust_code_mcp: "Self-hosted, optimized for 1M-10M LOC"
      claude_context: "Cloud-native, elastic scaling for millions of LOC"
      
    token_efficiency:
      projection: "rust-code-mcp may achieve 45-50% vs claude-context's 40%"
      reason: "True hybrid BM25+Vector+RRF vs vector-focused"
      validation: "Requires production benchmarking"
      
    incremental_indexing:
      both_projects: "Merkle tree approach for O(1) unchanged detection"
      rust_code_mcp_status: "Designed but not implemented"
      claude_context_status: "Production-deployed, millisecond-level"
      lesson: "Merkle trees are critical for large-scale efficiency"
      
    query_latency:
      rust_code_mcp_potential: "<100ms hybrid search (once Qdrant pipeline complete)"
      claude_context_proven: "<50ms p99 (Zilliz Cloud)"
      both_meet: "Real-time conversational AI requirements (<300ms)"
      
    architecture_maturity:
      rust_code_mcp: "All infrastructure in place, missing integration"
      claude_context: "End-to-end working system"
      gap: "Qdrant population pipeline is the critical blocker"
      
  recommendations:
    
    for_rust_code_mcp:
      priority_1: "Implement Qdrant population pipeline immediately"
      priority_2: "Run large-scale benchmarks (100k+ LOC)"
      priority_3: "Implement Merkle tree for >500k LOC optimization"
      priority_4: "Profile memory usage on real codebases"
      priority_5: "Validate 45-50% token reduction claim"
      
      validation_needed:
        - "1M LOC indexing in <5 min"
        - "Query latency <200ms p95 under load"
        - "Memory <4GB for 1M LOC"
        - "Incremental updates <5s for 1% change"
        - "NDCG@10 >0.75 retrieval quality"
        
    for_claude_context:
      documentation_improvement:
        - "Publish quantitative latency metrics"
        - "Document memory requirements per codebase size"
        - "Share maximum tested codebase sizes"
        - "Provide detailed performance breakdowns"
        
    general_observations:
      merkle_trees: "Essential for large-scale incremental indexing (both projects agree)"
      hybrid_search: "BM25 + vector superior to vector-only (both implement)"
      ast_chunking: "Semantic code understanding improves retrieval (both use)"
      cloud_vs_local: "Trade-off between elastic scalability and self-hosting"
      
  conclusion:
    rust_code_mcp_summary: >
      Ambitious, well-designed architecture targeting 1M+ LOC with <200ms p95 latency.
      All core infrastructure (Tantivy, Qdrant, tree-sitter, RRF) is in place.
      Main gap is Qdrant population pipeline integration. Once implemented, has potential
      to match or exceed claude-context's 40% token reduction due to true hybrid approach.
      Needs large-scale benchmarking to validate performance claims.
      
    claude_context_summary: >
      Production-proven system with validated 40% token reduction and <50ms p99 latency.
      Leverages Merkle trees for efficient incremental updates and Zilliz Cloud for
      elastic scalability. Limited published quantitative metrics, but real-world usage
      validates effectiveness. Cloud-native architecture enables handling "millions of LOC"
      with enterprise-grade reliability.
      
    competitive_position:
      rust_code_mcp_advantages:
        - "Self-hosted, no cloud dependencies"
        - "True hybrid (Tantivy BM25 + Qdrant vector + RRF)"
        - "Potential for 45-50% token reduction"
        - "Rust-native performance throughout stack"
        
      claude_context_advantages:
        - "Production-deployed, battle-tested"
        - "Cloud-native elastic scaling (>100M vectors)"
        - "Proven 40% token reduction"
        - "Merkle tree implementation working"
        
      market_differentiation:
        rust_code_mcp: "Best for self-hosted, Rust-focused, 1M-10M LOC codebases"
        claude_context: "Best for cloud-native, multi-language, enterprise-scale deployments"
        
    next_steps:
      immediate: "Implement Qdrant population pipeline in rust-code-mcp"
      short_term: "Benchmark rust-code-mcp on 100k+ LOC codebases"
      medium_term: "Implement Merkle tree optimization"
      long_term: "Validate token reduction and scalability claims in production"

metadata:
  research_methodology:
    - "Explored rust-code-mcp codebase using Task agent (Explore subagent)"
    - "Reviewed all documentation in docs/ directory"
    - "Analyzed code comments and configuration files"
    - "Web research for claude-context performance data"
    - "Cross-referenced local comparison documentation"
    
  confidence_levels:
    rust_code_mcp_claims: "High confidence in design targets, low confidence in actual performance (not benchmarked at scale)"
    claude_context_claims: "High confidence in 40% token reduction (production-verified), medium confidence in other metrics (limited quantitative data)"
    
  data_sources:
    rust_code_mcp:
      - "/home/molaco/Documents/rust-code-mcp/README.md"
      - "/home/molaco/Documents/rust-code-mcp/docs/NEW_PLAN.md"
      - "/home/molaco/Documents/rust-code-mcp/docs/DEEP_RESEARCH_FINDINGS.md"
      - "/home/molaco/Documents/rust-code-mcp/docs/INDEXING_STRATEGIES.md"
      - "/home/molaco/Documents/rust-code-mcp/docs/PHASE6_COMPLETE.md"
      - "/home/molaco/Documents/rust-code-mcp/TESTING.md"
      - "/home/molaco/Documents/rust-code-mcp/src/vector_store/mod.rs"
      
    claude_context:
      - "Web: github.com/zilliztech/claude-context"
      - "/home/molaco/Documents/rust-code-mcp/docs/COMPARISON_CLAUDE_CONTEXT.md"
      - "/home/molaco/Documents/rust-code-mcp/docs/DEEP_RESEARCH_FINDINGS.md"
      - "/home/molaco/Documents/rust-code-mcp/docs/ADVANCED_RESEARCH.md"
      
  limitations:
    - "No access to claude-context source code for direct analysis"
    - "rust-code-mcp performance targets are projections, not benchmarked at scale"
    - "claude-context metrics mostly qualitative from public sources"
    - "Memory usage data limited for both projects"
    - "No head-to-head benchmarks on identical codebases"