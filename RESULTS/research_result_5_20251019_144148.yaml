document_title: "Code Chunking Strategy Comparison: rust-code-mcp vs claude-context"
date: "2025-10-19"
author: "Research Analysis"
version: "1.0"
---

executive_summary:
  conclusion: |
    Both rust-code-mcp and claude-context use tree-sitter for AST-based semantic chunking,
    but with significantly different approaches and implementation depth. rust-code-mcp employs
    a pure symbol-based strategy extracting complete semantic units, while claude-context uses
    a hybrid AST-first approach with sophisticated fallback mechanisms.
  
  key_findings:
    - Both systems leverage tree-sitter for AST parsing
    - rust-code-mcp uses pure symbol-based chunking (one symbol = one chunk)
    - claude-context uses character-based chunking with AST node boundaries
    - claude-context has dual fallback (AST → LangChain text splitter)
    - rust-code-mcp has richer context enrichment (imports, calls, docstrings)
    - Different chunk sizing strategies (symbols vs character limits)

---

# RUST-CODE-MCP CHUNKING STRATEGY

implementation:
  approach: "Pure AST-based symbol extraction"
  framework: "tree-sitter-rust"
  chunking_unit: "Semantic symbols (functions, structs, traits, impls, etc.)"
  
parsing_implementation:
  location: "src/parser/mod.rs"
  parser_type: "RustParser (tree-sitter based)"
  
  supported_symbol_types:
    - name: "Function"
      node_kind: "function_item"
      metadata: ["is_async", "is_unsafe", "is_const"]
      extraction_method: "extract_function"
      
    - name: "Struct"
      node_kind: "struct_item"
      extraction_method: "extract_struct"
      
    - name: "Enum"
      node_kind: "enum_item"
      extraction_method: "extract_enum"
      
    - name: "Trait"
      node_kind: "trait_item"
      extraction_method: "extract_trait"
      
    - name: "Impl Block"
      node_kind: "impl_item"
      metadata: ["trait_name", "type_name"]
      extraction_method: "extract_impl"
      
    - name: "Module"
      node_kind: "mod_item"
      extraction_method: "extract_module"
      
    - name: "Const"
      node_kind: "const_item"
      extraction_method: "extract_const"
      
    - name: "Static"
      node_kind: "static_item"
      extraction_method: "extract_static"
      
    - name: "Type Alias"
      node_kind: "type_item"
      extraction_method: "extract_type_alias"

chunking_implementation:
  location: "src/chunker/mod.rs"
  chunker_type: "Symbol-based semantic chunker"
  
  chunk_structure:
    id: "ChunkId (UUID v4)"
    content: "Full source code of the symbol"
    context: "ChunkContext with rich metadata"
    overlap_prev: "Optional overlap from previous chunk"
    overlap_next: "Optional overlap to next chunk"
  
  chunking_parameters:
    granularity: "Per symbol (variable size)"
    overlap_type: "Line-based between adjacent symbols"
    overlap_percentage: "20% (default, configurable)"
    overlap_method: "calculate_overlap() - takes first/last N lines"
  
  chunk_boundaries:
    strategy: "Natural symbol boundaries from AST"
    method: "extract_symbol_code() using symbol.range (start_line, end_line)"
    size_constraint: "None - chunks match complete symbol size"
    size_variability: "High - from single-line consts to multi-hundred-line functions"

context_enrichment:
  approach: "Contextual Retrieval Pattern (Anthropic)"
  implementation: "format_for_embedding()"
  location: "src/chunker/mod.rs:76-142"
  
  metadata_components:
    file_context:
      - "File path"
      - "Line range (start-end)"
      
    module_context:
      - "Module path (extracted from file path)"
      - "Format: crate::module::submodule"
      
    symbol_context:
      - "Symbol name"
      - "Symbol kind (function, struct, trait, etc.)"
      
    documentation:
      - "Docstring (/// or //! comments)"
      - "Extracted by extract_docstring_before()"
      
    import_context:
      - "Import statements from file"
      - "Limit: First 5 imports shown"
      - "Source: extract_imports() from parser"
      
    call_graph_context:
      - "Outgoing function calls"
      - "Limit: First 5 calls shown"
      - "Source: CallGraph::get_callees()"
  
  formatted_output_example: |
    // File: src/parser/mod.rs
    // Location: lines 130-145
    // Module: crate::parser
    // Symbol: parse_file (function)
    // Purpose: Parse a Rust source file and extract symbols
    // Imports: std::fs, std::path::Path, tree_sitter::Parser
    // Calls: fs::read_to_string, parse_source
    
    pub fn parse_file(&mut self, path: &Path) -> Result<Vec<Symbol>, Box<dyn std::error::Error>> {
        let source = fs::read_to_string(path)?;
        self.parse_source(&source)
    }

parsing_workflow:
  steps:
    1_parse:
      description: "Parse source code with tree-sitter"
      method: "parser.parse(source, None)"
      output: "Tree (AST)"
      
    2_traverse:
      description: "Recursively traverse AST nodes"
      method: "traverse_node()"
      pattern: "Match on node.kind()"
      
    3_extract:
      description: "Extract symbols at matched nodes"
      extracts:
        - "Symbol metadata (name, kind, visibility)"
        - "Line range (start_line, end_line, start_byte, end_byte)"
        - "Docstring from preceding comments"
        - "Function modifiers (async, unsafe, const)"
      
    4_build_graph:
      description: "Build call graph and extract imports"
      methods:
        - "CallGraph::build(tree, source)"
        - "extract_imports(tree, source)"
        - "TypeReferenceTracker::build(tree, source)"
      
    5_chunk:
      description: "Create chunks from symbols"
      method: "chunk_file(file_path, source, parse_result)"
      process:
        - "For each symbol, extract code using line range"
        - "Build ChunkContext with imports and calls"
        - "Create CodeChunk with UUID"
        - "Add overlap between adjacent chunks"

additional_features:
  call_graph:
    location: "src/parser/call_graph.rs"
    purpose: "Track function call relationships"
    usage: "Enrich chunk context with outgoing calls"
    
  import_extraction:
    location: "src/parser/imports.rs"
    purpose: "Extract import statements and dependencies"
    usage: "Add import context to chunks"
    
  type_references:
    location: "src/parser/type_references.rs"
    purpose: "Track type usage and references"
    usage: "Understand type relationships"

language_support:
  current: ["Rust only"]
  extensibility: "Designed for extension but only Rust implemented"
  limitation: "Single-language parser (RustParser)"

fallback_mechanism:
  exists: false
  note: "No fallback to text-based chunking if AST parsing fails"
  implication: "Parsing failures would cause indexing failures"

---

# CLAUDE-CONTEXT CHUNKING STRATEGY

implementation:
  approach: "Hybrid AST-first with text-based fallback"
  framework: "tree-sitter (multi-language)"
  chunking_unit: "Character-bounded semantic nodes"
  
parsing_implementation:
  location: "packages/core/src/splitter/ast-splitter.ts"
  file_size: "11,235 bytes"
  parser_type: "tree-sitter Parser with language-specific grammars"
  
  supported_languages:
    count: 10
    list:
      - name: "JavaScript"
        grammar: "tree-sitter-javascript"
      - name: "TypeScript"
        grammar: "tree-sitter-typescript"
      - name: "Python"
        grammar: "tree-sitter-python"
      - name: "Java"
        grammar: "tree-sitter-java"
      - name: "C++"
        grammar: "tree-sitter-cpp"
      - name: "C"
        grammar: "tree-sitter-c"
      - name: "Go"
        grammar: "tree-sitter-go"
      - name: "Rust"
        grammar: "tree-sitter-rust"
      - name: "C#"
        grammar: "tree-sitter-csharp"
      - name: "Scala"
        grammar: "tree-sitter-scala"

chunking_implementation:
  primary_splitter:
    name: "AstCodeSplitter"
    location: "packages/core/src/splitter/ast-splitter.ts"
    
    parameters:
      chunk_size:
        default: 2500
        unit: "characters"
        configurable: true
        setter: "setChunkSize(chunkSize: number)"
        
      overlap:
        default: 300
        unit: "characters"
        configurable: true
        setter: "setChunkOverlap(chunkOverlap: number)"
  
  fallback_splitter:
    name: "LangChainCodeSplitter"
    location: "packages/core/src/splitter/langchain-splitter.ts"
    file_size: "4,792 bytes"
    library: "langchain/text_splitter (RecursiveCharacterTextSplitter)"
    
    parameters:
      chunk_size:
        default: 1000
        unit: "characters"
        
      overlap:
        default: 200
        unit: "characters"
    
    supported_languages:
      count: "20+"
      includes:
        - JavaScript/TypeScript
        - Python
        - Java
        - C/C++
        - Go
        - Rust
        - PHP
        - Ruby
        - Swift
        - Scala
        - HTML
        - Markdown
        - LaTeX
        - Solidity
        - "and more..."

chunk_structure:
  type: "CodeChunk interface"
  fields:
    content: "String - the code text"
    language: "String - detected language"
    filePath: "String - source file path"
    metadata: "Object - additional context"

chunking_boundaries:
  strategy: "AST-node-based with character limits"
  method: "extractChunks() - traverse AST for splittable nodes"
  
  size_constraint:
    limit: "2,500 characters (default)"
    enforcement: "refineChunks() splits oversized chunks by lines"
    
  size_variability: "Medium - bounded by max chunk size"
  
  node_types_targeted:
    - "function_declaration"
    - "function_definition"
    - "class_declaration"
    - "class_definition"
    - "method_declaration"
    - "method_definition"
    - "interface declarations"
    - "module/namespace declarations"

workflow:
  ast_splitter:
    steps:
      1_parse:
        method: "Parser.parse(code)"
        output: "Tree (AST with root node)"
        
      2_extract:
        method: "extractChunks()"
        process: "Traverse nodes, match splittable types"
        output: "Array of chunks from AST nodes"
        
      3_refine:
        method: "refineChunks()"
        purpose: "Handle oversized chunks"
        strategy: "Line-based splitting if chunk > chunk_size"
        
      4_overlap:
        method: "addOverlap()"
        strategy: "Append last N characters from previous chunk"
        amount: "300 characters (default)"
        
      5_return:
        output: "CodeChunk[] with complete metadata"

fallback_mechanism:
  exists: true
  levels: 2
  
  level_1_unsupported_language:
    trigger: "Language not in 10-language AST parser list"
    action: "Fall back to LangChainCodeSplitter"
    method: "Check isLanguageSupported(language)"
    
  level_2_parsing_failure:
    trigger: "tree-sitter parsing fails or tree.rootNode is null"
    action: "Fall back to LangChainCodeSplitter"
    logging: "console.log('AST parsing failed, falling back...')"
    
  langchain_splitter_behavior:
    algorithm: "RecursiveCharacterTextSplitter"
    strategy: "Language-aware separators (when available)"
    parameters: "1000 char chunks, 200 char overlap"
    output: "Text-based chunks with estimated metadata"

context_enrichment:
  approach: "Minimal compared to rust-code-mcp"
  metadata_included:
    - "File path"
    - "Language"
    - "Line ranges (estimated)"
  
  not_included:
    - "No import extraction"
    - "No call graph analysis"
    - "No docstring extraction"
    - "No module path derivation"
  
  note: |
    claude-context relies on the embedding model and vector database to capture
    semantic relationships, rather than explicit metadata enrichment

language_support:
  primary: "10 languages via tree-sitter"
  fallback: "20+ languages via LangChain"
  total_coverage: "30+ languages with varying quality"
  extensibility: "Easy to add new tree-sitter parsers"

production_features:
  configuration:
    runtime_updates: true
    interface: "Splitter interface with setters"
    hot_swapping: "updateSplitter() method"
    
  robustness:
    error_handling: "Graceful degradation via fallback"
    never_fails: "Always returns chunks (AST or text)"
    
  optimization:
    incremental: "Merkle tree change detection"
    batch_processing: "Efficient multi-file handling"

---

# COMPARATIVE ANALYSIS

chunking_philosophy:
  rust_code_mcp:
    paradigm: "Pure semantic chunking"
    principle: "One symbol = One chunk"
    size_strategy: "Natural symbol boundaries (variable size)"
    quality: "High semantic coherence"
    trade_off: "Variable chunk sizes, potential for very large chunks"
    
  claude_context:
    paradigm: "Bounded semantic chunking"
    principle: "Extract AST nodes within character limits"
    size_strategy: "Character-bounded (2,500 chars max)"
    quality: "Balanced semantic coherence + size control"
    trade_off: "May split large functions, but predictable sizes"

ast_parsing:
  similarities:
    - "Both use tree-sitter for AST parsing"
    - "Both extract complete semantic units"
    - "Both preserve code structure"
    
  differences:
    rust_code_mcp:
      depth: "Deep Rust-specific parsing"
      extractors: "9 symbol-specific extraction methods"
      metadata: "Rich (visibility, modifiers, docstrings)"
      
    claude_context:
      breadth: "Multi-language support (10 languages)"
      extractors: "Generic splittable node matching"
      metadata: "Basic (path, language, lines)"

chunk_sizing:
  rust_code_mcp:
    method: "Natural symbol boundaries"
    constraints: "None"
    range: "Single line to hundreds of lines"
    overlap: "20% line-based between adjacent symbols"
    
    example_sizes:
      small_const: "1-2 lines"
      simple_function: "5-20 lines"
      complex_impl: "50-200+ lines"
      large_module: "Potentially entire file"
    
  claude_context:
    method: "Character-limited AST nodes"
    constraints: "2,500 characters default (AST), 1,000 (fallback)"
    range: "Up to max chunk size, then split by lines"
    overlap: "300 characters appended from previous chunk"
    
    example_sizes:
      typical: "~50-100 lines (varies by code density)"
      maximum: "Never exceeds 2,500 characters"
      refined: "Oversized chunks split by refineChunks()"

context_enrichment:
  rust_code_mcp:
    richness: "Very High"
    components:
      file_metadata: ["path", "line_range"]
      module_metadata: ["module_path"]
      symbol_metadata: ["name", "kind", "visibility"]
      documentation: ["docstrings"]
      dependencies: ["imports (top 5)"]
      relationships: ["outgoing_calls (top 5)"]
    
    format_for_embedding:
      structured: true
      comments_prefix: "// File: ... // Location: ... // Symbol: ..."
      benefit: "Explicit context for embedding model"
      inspiration: "Anthropic contextual retrieval pattern"
    
  claude_context:
    richness: "Low to Medium"
    components:
      file_metadata: ["path", "language"]
      line_metadata: ["estimated_line_ranges"]
    
    rationale: |
      Relies on embedding model's semantic understanding rather than
      explicit metadata. Trades off metadata richness for multi-language
      generality and implementation simplicity.

fallback_strategy:
  rust_code_mcp:
    exists: false
    behavior_on_failure: "Parsing errors propagate as indexing failures"
    robustness: "Lower - single point of failure"
    scope: "Rust only, no fallback for other languages or parse errors"
    
  claude_context:
    exists: true
    levels: 2
    
    fallback_chain:
      1_primary: "AST splitter (10 languages)"
      2_secondary: "LangChain splitter (20+ languages)"
      3_tertiary: "Generic text splitter (any content)"
    
    behavior_on_failure: "Graceful degradation to text-based chunking"
    robustness: "High - never fails to produce chunks"
    scope: "30+ languages with automatic selection"

language_support:
  rust_code_mcp:
    count: 1
    languages: ["Rust"]
    extensibility: "Requires implementing new parser per language"
    effort: "High - need symbol extraction for each language"
    
  claude_context:
    ast_count: 10
    ast_languages:
      - JavaScript
      - TypeScript
      - Python
      - Java
      - C/C++
      - Go
      - Rust
      - C#
      - Scala
      
    fallback_count: "20+"
    extensibility: "Easy - add tree-sitter grammar or LangChain config"
    effort: "Low - generic splittable node matching"

chunk_quality:
  semantic_completeness:
    rust_code_mcp:
      score: "Very High"
      reason: "Always complete symbols, never split mid-function"
      guarantee: "100% semantic coherence"
      
    claude_context:
      score: "High (with exceptions)"
      reason: "Usually complete nodes, but splits large functions"
      guarantee: "Best-effort within size limits"
  
  size_predictability:
    rust_code_mcp:
      score: "Low"
      reason: "Variable from 1 line to entire files"
      implication: "May create very large chunks for big symbols"
      
    claude_context:
      score: "High"
      reason: "Bounded by character limits with refinement"
      implication: "Consistent chunk sizes, better for batch processing"
  
  overlap_quality:
    rust_code_mcp:
      method: "Line-based (20%)"
      calculation: "Take first/last N lines between adjacent symbols"
      benefit: "Maintains context across symbol boundaries"
      
    claude_context:
      method: "Character-based (300 chars)"
      calculation: "Append last 300 characters from previous chunk"
      benefit: "Smooth context transition, prevents context gaps"

embedding_integration:
  rust_code_mcp:
    preparation: "format_for_embedding()"
    format: |
      Structured comment prefix + actual code
      Example:
      // File: src/foo.rs
      // Location: lines 10-20
      // Symbol: bar (function)
      // Imports: std::io
      // Calls: helper
      
      fn bar() { ... }
    
    embedding_input: "Formatted string with metadata"
    model: "fastembed (local, all-MiniLM-L6-v2, 384d)"
    
  claude_context:
    preparation: "Direct chunk content"
    format: "Raw code without metadata prefix"
    embedding_input: "Chunk.content as-is"
    models:
      - "OpenAI text-embedding-3-large (3072d)"
      - "Voyage AI voyage-code-3 (specialized)"
      - "Ollama (local models)"

production_readiness:
  rust_code_mcp:
    maturity: "Development/Testing"
    error_handling: "Basic - errors propagate"
    configurability: "Medium - overlap adjustable"
    monitoring: "Limited"
    
  claude_context:
    maturity: "Production-deployed"
    error_handling: "Robust - dual fallback"
    configurability: "High - runtime adjustable"
    monitoring: "Progress callbacks, logging"

---

# KEY DIFFERENCES SUMMARY

fundamental_approach:
  dimension: "Chunking Granularity"
  
  rust_code_mcp:
    strategy: "Symbol-aligned, variable size"
    unit: "Complete semantic symbol"
    size: "Natural (unbounded)"
    
  claude_context:
    strategy: "Character-bounded, AST-guided"
    unit: "AST nodes within size limits"
    size: "Controlled (2,500 char max)"

context_philosophy:
  dimension: "Metadata Enrichment"
  
  rust_code_mcp:
    approach: "Explicit context injection"
    method: "Structured metadata comments"
    benefit: "Rich context for smaller models"
    trade_off: "Rust-specific, manual enrichment"
    
  claude_context:
    approach: "Implicit semantic understanding"
    method: "Rely on embedding model capability"
    benefit: "Multi-language generality"
    trade_off: "Requires high-quality embeddings"

robustness:
  dimension: "Failure Handling"
  
  rust_code_mcp:
    strategy: "Fail-fast"
    fallback: "None"
    implication: "Parse errors stop indexing"
    
  claude_context:
    strategy: "Graceful degradation"
    fallback: "Two-level (AST → LangChain)"
    implication: "Always produces chunks"

language_scope:
  dimension: "Multi-language Support"
  
  rust_code_mcp:
    focus: "Single language, deep analysis"
    languages: "Rust only"
    depth: "Very deep (9 symbol types, modifiers, etc.)"
    
  claude_context:
    focus: "Multi-language, broad coverage"
    languages: "30+ (10 AST, 20+ text)"
    depth: "Medium (generic splittable nodes)"

implementation_complexity:
  rust_code_mcp:
    parser: "Custom RustParser with symbol-specific extractors"
    chunker: "Symbol-based chunker with overlap calculation"
    enrichment: "Call graph, imports, type references"
    complexity: "High for single language"
    
  claude_context:
    parser: "Generic tree-sitter wrapper"
    chunker: "Character-bounded AST + text fallback"
    enrichment: "Minimal (path, language)"
    complexity: "Medium for 10 languages"

---

# ARCHITECTURAL RECOMMENDATIONS

when_to_use_rust_code_mcp_approach:
  scenarios:
    - "Deep single-language code analysis"
    - "Need complete symbol extraction"
    - "Require call graph and import relationships"
    - "Using local/smaller embedding models"
    - "Prioritize semantic completeness over size"
    
  benefits:
    - "100% semantic coherence guaranteed"
    - "Rich context for retrieval"
    - "Complete symbol information"
    - "Natural code boundaries"
    
  limitations:
    - "Rust only (currently)"
    - "No fallback if parsing fails"
    - "Variable chunk sizes"
    - "Higher implementation complexity per language"

when_to_use_claude_context_approach:
  scenarios:
    - "Multi-language codebase indexing"
    - "Need predictable chunk sizes"
    - "Require production robustness"
    - "Using high-quality cloud embeddings"
    - "Want simple, extensible architecture"
    
  benefits:
    - "30+ languages supported"
    - "Graceful error handling"
    - "Predictable memory/compute"
    - "Battle-tested in production"
    
  limitations:
    - "May split large symbols"
    - "Less context enrichment"
    - "Character-based sizing may cut semantics"
    - "Relies on embedding quality"

hybrid_approach_possibilities:
  concept: "Combine best of both strategies"
  
  option_1_adaptive_sizing:
    strategy: "Use symbol boundaries but enforce max size"
    implementation: |
      1. Extract symbols via AST (rust-code-mcp style)
      2. If symbol > max_size, split by nested nodes
      3. Add rich context (rust-code-mcp enrichment)
      4. Fall back to text splitter on parse failure (claude-context style)
    
    benefits:
      - "Semantic coherence where possible"
      - "Size predictability"
      - "Rich context"
      - "Robustness"
  
  option_2_tiered_enrichment:
    strategy: "Language-specific enrichment with generic fallback"
    implementation: |
      1. For supported languages (Rust, Python, etc.):
         Use deep parsing with call graphs, imports, etc.
      2. For other languages:
         Use AST-only chunking (claude-context style)
      3. For all chunks:
         Apply format_for_embedding() with available metadata
    
    benefits:
      - "Best quality for priority languages"
      - "Broad language coverage"
      - "Consistent embedding format"

---

# PERFORMANCE IMPLICATIONS

chunk_count_estimates:
  scenario: "100k LOC codebase"
  
  rust_code_mcp:
    estimation: "~3,000-5,000 chunks"
    reasoning: "Avg 20-30 LOC per symbol"
    variability: "High - depends on code structure"
    
  claude_context:
    estimation: "~8,000-12,000 chunks"
    reasoning: "2,500 char ≈ 60-80 LOC, may split large symbols"
    variability: "Low - bounded by character limit"

embedding_generation:
  rust_code_mcp:
    input_size: "Larger - includes metadata prefix"
    example: "~100-500 tokens per chunk (metadata + code)"
    overhead: "+20-30% tokens from metadata"
    benefit: "Better retrieval accuracy"
    
  claude_context:
    input_size: "Smaller - raw code only"
    example: "~50-400 tokens per chunk"
    overhead: "Minimal"
    benefit: "Faster embedding, lower cost"

retrieval_quality:
  rust_code_mcp:
    advantages:
      - "Explicit context improves relevance"
      - "Call graph enables relationship queries"
      - "Import tracking for dependency searches"
      
    potential_issues:
      - "Large chunks may dilute precision"
      - "Variable sizes affect scoring"
      
  claude_context:
    advantages:
      - "Consistent chunk sizes improve ranking"
      - "Overlap prevents context gaps"
      - "Proven 40% token reduction"
      
    potential_issues:
      - "Split symbols may harm semantic retrieval"
      - "Less explicit context"

---

# TESTING & VALIDATION REFERENCES

rust_code_mcp:
  test_files:
    - location: "src/parser/mod.rs:582-808"
      coverage:
        - "Symbol extraction (functions, structs, traits)"
        - "Docstring parsing"
        - "Impl block detection"
        - "Async/unsafe modifiers"
        
    - location: "src/chunker/mod.rs:330-485"
      coverage:
        - "Chunk creation"
        - "format_for_embedding()"
        - "Module path extraction"
        - "Symbol-based chunking"
        - "Overlap calculation"
  
  verified_features:
    - "Tree-sitter parsing works"
    - "Symbol extraction successful"
    - "Docstrings captured"
    - "Call graph built correctly"
    - "Chunks formatted with metadata"

claude_context:
  production_metrics:
    token_reduction: "40% vs grep-only"
    deployment: "Multiple organizations"
    languages_tested: "10 (AST) + 20+ (fallback)"
    
  published_benchmarks:
    change_detection: "Milliseconds (Merkle)"
    use_case_improvement: "3-300x faster vs grep"
    
  validation_sources:
    - "GitHub: zilliztech/claude-context"
    - "Blog: Zilliz 'Against Claude Code grep-only'"
    - "Technical: Build Code Retrieval for CC (zc277584121)"

---

# CONCLUSION & INSIGHTS

primary_insight:
  statement: |
    rust-code-mcp and claude-context represent two valid but divergent approaches
    to AST-based code chunking. rust-code-mcp prioritizes semantic purity and
    rich context enrichment at the cost of language support and size variability.
    claude-context prioritizes robustness, multi-language breadth, and size
    predictability at the cost of reduced context enrichment.

strategic_recommendation:
  for_rust_code_mcp:
    immediate:
      - "Add fallback to text-based chunking (like claude-context)"
      - "Consider max chunk size limit with nested splitting"
      - "Keep rich context enrichment as differentiator"
      
    medium_term:
      - "Add 2-3 more languages (Python, TypeScript, Go)"
      - "Use same deep enrichment for each"
      - "Generic AST fallback for others"
      
    long_term:
      - "Hybrid approach: deep parsing + size controls"
      - "Configurable strategies per use case"

  for_understanding_trade_offs:
    semantic_purity_vs_size_control:
      rust_code_mcp_position: "Pure semantics, variable size"
      claude_context_position: "Bounded semantics, controlled size"
      ideal: "Adaptive - semantic where possible, split when needed"
      
    explicit_vs_implicit_context:
      rust_code_mcp_position: "Inject metadata into embeddings"
      claude_context_position: "Trust embedding model semantics"
      ideal: "Tier by model quality - rich for small, lean for large"
      
    depth_vs_breadth:
      rust_code_mcp_position: "Deep single-language analysis"
      claude_context_position: "Broad multi-language coverage"
      ideal: "Deep for priority languages, generic for rest"

final_assessment:
  rust_code_mcp_strengths:
    - "Symbol-level semantic precision"
    - "Anthropic contextual retrieval pattern"
    - "Call graph and dependency tracking"
    - "Complete symbol extraction"
    
  rust_code_mcp_weaknesses:
    - "No fallback mechanism"
    - "Single language only"
    - "Unbounded chunk sizes"
    - "Higher complexity per language"
    
  claude_context_strengths:
    - "Production-proven robustness"
    - "Multi-language (30+)"
    - "Graceful degradation"
    - "Predictable performance"
    
  claude_context_weaknesses:
    - "May split large symbols"
    - "Limited context enrichment"
    - "Less fine-grained symbol tracking"
    
  complementary_aspects:
    observation: |
      The two implementations are highly complementary. rust-code-mcp's
      rich context enrichment could enhance claude-context's multi-language
      chunks. claude-context's fallback robustness could make rust-code-mcp
      production-ready. A hybrid combining both would be optimal.

---

# REFERENCE IMPLEMENTATION LOCATIONS

rust_code_mcp:
  parser:
    - "src/parser/mod.rs (808 lines) - RustParser with tree-sitter"
    - "src/parser/call_graph.rs - Function call tracking"
    - "src/parser/imports.rs - Import extraction"
    - "src/parser/type_references.rs - Type usage tracking"
    
  chunker:
    - "src/chunker/mod.rs (486 lines) - Symbol-based chunking"
    
  embeddings:
    - "src/embeddings/mod.rs - format_for_embedding() integration"
    
  search:
    - "src/search/mod.rs - Hybrid search with enriched chunks"

claude_context:
  splitters:
    - "packages/core/src/splitter/ast-splitter.ts (11,235 bytes)"
    - "packages/core/src/splitter/langchain-splitter.ts (4,792 bytes)"
    - "packages/core/src/splitter/index.ts (1,132 bytes)"
    
  orchestration:
    - "packages/core/src/context.ts (49,276 bytes) - Main orchestration"
    
  parsers:
    - "Uses tree-sitter npm packages (tree-sitter-{language})"

---

metadata:
  analysis_date: "2025-10-19"
  rust_code_mcp_version: "Current (from repository)"
  claude_context_version: "v1.x (from GitHub zilliztech/claude-context)"
  research_depth: "Deep - source code analysis + documentation"
  confidence_level: "High - based on actual implementation review"