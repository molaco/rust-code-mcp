---
title: "Hybrid Search Implementation Analysis - rust-code-mcp"
date: "2025-10-22"
codebase: "rust-code-mcp (NOT claude-context)"
note: |
  This analysis documents the rust-code-mcp hybrid search system.
  It uses Qdrant (not Milvus) and Tantivy for BM25 (not a custom implementation).

# ==============================================================================
# EXECUTIVE SUMMARY
# ==============================================================================
executive_summary:
  description: |
    rust-code-mcp implements a sophisticated hybrid search system that combines:
    1. BM25 lexical search using Tantivy (full-text search engine)
    2. Dense vector search using Qdrant (vector database)
    3. Reciprocal Rank Fusion (RRF) for result merging

    The system is designed for code search with semantic understanding and
    resilient fallback mechanisms.

  key_technologies:
    bm25_engine: "Tantivy (Rust native full-text search library)"
    vector_database: "Qdrant (vector search engine)"
    embedding_model: "all-MiniLM-L6-v2 (384 dimensions)"
    fusion_algorithm: "Reciprocal Rank Fusion (RRF)"

  architecture_highlights:
    - "Dual indexing: Tantivy for BM25, Qdrant for vectors"
    - "Parallel search execution using tokio::join!"
    - "Automatic fallback when components fail"
    - "Configurable RRF k parameter with automatic tuning"
    - "NDCG-based evaluation metrics"

# ==============================================================================
# FILE STRUCTURE
# ==============================================================================
file_structure:
  search_module:
    location: "/home/molaco/Documents/rust-code-mcp/src/search/"
    files:
      - path: "src/search/mod.rs"
        lines: 550
        purpose: "Core hybrid search implementation with RRF algorithm"

      - path: "src/search/bm25.rs"
        lines: 304
        purpose: "BM25 search wrapper around Tantivy index"

      - path: "src/search/rrf_tuner.rs"
        lines: 447
        purpose: "RRF parameter tuning with NDCG evaluation"

      - path: "src/search/resilient.rs"
        lines: 245
        purpose: "Resilient search with automatic fallback mechanisms"

  vector_store:
    location: "/home/molaco/Documents/rust-code-mcp/src/vector_store/mod.rs"
    lines: 548
    purpose: "Qdrant vector database integration"

  schema:
    location: "/home/molaco/Documents/rust-code-mcp/src/schema.rs"
    lines: 241
    purpose: "Tantivy schemas for file and chunk indexing"

  search_tool:
    location: "/home/molaco/Documents/rust-code-mcp/src/tools/search_tool.rs"
    lines: 1001
    purpose: "MCP tool interface for search operations"

# ==============================================================================
# BM25 SEARCH IMPLEMENTATION
# ==============================================================================
bm25_search:
  implementation:
    file: "src/search/bm25.rs"
    engine: "Tantivy"
    description: |
      BM25 search is implemented as a wrapper around Tantivy, a full-featured
      full-text search library written in Rust. It provides lexical matching
      with BM25 ranking algorithm.

  key_components:
    Bm25Search_struct:
      location: "src/search/bm25.rs:17-21"
      fields:
        - "index: Index - Tantivy index instance"
        - "schema: ChunkSchema - Schema definition"
        - "reader: IndexReader - Index reader for searching"

  search_fields:
    location: "src/search/bm25.rs:75-81"
    indexed_fields:
      - field: "content"
        description: "Main code content"
      - field: "symbol_name"
        description: "Function/struct/trait names"
      - field: "docstring"
        description: "Documentation comments"

  code_example:
    location: "src/search/bm25.rs:67-121"
    snippet: |
      pub fn search(
          &self,
          query: &str,
          limit: usize,
      ) -> Result<Vec<(ChunkId, f32, CodeChunk)>, Box<dyn std::error::Error + Send>> {
          let searcher = self.reader.searcher();

          // Parse query across multiple fields
          let query_parser = QueryParser::for_index(
              &self.index,
              vec![
                  self.schema.content,      // Main code content
                  self.schema.symbol_name,  // Symbol names
                  self.schema.docstring,    // Documentation
              ],
          );

          let query = query_parser.parse_query(query)?;

          // Search with limit
          let top_docs = searcher.search(&query, &TopDocs::with_limit(limit))?;

          // Convert Tantivy results to (ChunkId, score, CodeChunk)
          // ...
      }

  schema_definition:
    file: "src/schema.rs:79-179"
    fields:
      - name: "chunk_id"
        type: "text"
        indexed: true
        stored: true
        description: "UUID for chunk identification"

      - name: "content"
        type: "text"
        indexed: true
        stored: true
        description: "Code content with positions"

      - name: "symbol_name"
        type: "text"
        indexed: true
        stored: true
        description: "Symbol names (functions, structs, etc.)"

      - name: "docstring"
        type: "text"
        indexed: true
        stored: true
        description: "Documentation strings"

      - name: "chunk_json"
        type: "text"
        stored: true
        indexed: false
        description: "Full CodeChunk serialized as JSON"

# ==============================================================================
# VECTOR SEARCH IMPLEMENTATION
# ==============================================================================
vector_search:
  implementation:
    file: "src/vector_store/mod.rs"
    database: "Qdrant"
    description: |
      Vector search uses Qdrant, a high-performance vector search engine.
      Embeddings are generated using the all-MiniLM-L6-v2 model (384 dimensions).

  key_components:
    VectorStore_struct:
      location: "src/vector_store/mod.rs:55-60"
      fields:
        - "client: Arc<QdrantClient> - Qdrant client"
        - "collection_name: String - Collection identifier"
        - "vector_size: usize - Embedding dimensions (384)"

    configuration:
      location: "src/vector_store/mod.rs:19-52"
      default_config:
        url: "http://localhost:6334"
        collection_name: "code_chunks_{project_name}"
        vector_size: 384
        distance_metric: "Cosine"

  search_implementation:
    location: "src/vector_store/mod.rs:231-291"
    snippet: |
      pub async fn search(
          &self,
          query_vector: Embedding,
          limit: usize,
      ) -> Result<Vec<SearchResult>, Box<dyn std::error::Error + Send>> {
          let search_points = SearchPoints {
              collection_name: self.collection_name.clone(),
              vector: query_vector,
              limit: limit as u64,
              with_payload: Some(true.into()),
              ..Default::default()
          };

          let response = self.client.search_points(search_points).await?;

          // Convert Qdrant results to SearchResult
          // Deserialize CodeChunk from payload
          // ...
      }

  hnsw_configuration:
    location: "src/vector_store/mod.rs:131-151"
    default_params:
      m: 16
      ef_construct: 100
      full_scan_threshold: 10000
      max_indexing_threads: 4
    description: |
      HNSW (Hierarchical Navigable Small World) graph parameters for
      efficient approximate nearest neighbor search.

  embedding_integration:
    location: "src/search/mod.rs:80-91"
    workflow:
      - step: 1
        action: "Generate query embedding using EmbeddingGenerator"
        code: "let query_embedding = self.embedding_generator.embed(query)?;"
      - step: 2
        action: "Search Qdrant with embedding vector"
        code: "self.vector_store.search(query_embedding, limit).await"
      - step: 3
        action: "Return results with similarity scores"

# ==============================================================================
# RECIPROCAL RANK FUSION (RRF) ALGORITHM
# ==============================================================================
rrf_algorithm:
  description: |
    Reciprocal Rank Fusion (RRF) is a simple yet effective method for combining
    rankings from multiple information retrieval systems. It uses rank positions
    rather than raw scores, making it robust to score scale differences.

  formula:
    mathematical: "score(item) = sum(weight_i / (k + rank_i)) for all systems i"
    explanation: |
      - k: constant parameter (typically 60)
      - rank_i: 1-indexed rank in system i
      - weight_i: importance weight for system i

  implementation:
    primary_location: "src/search/mod.rs:196-263"
    secondary_location: "src/search/mod.rs:288-353"

  code_walkthrough:
    location: "src/search/mod.rs:196-263"
    snippet: |
      fn reciprocal_rank_fusion_with_k(
          &self,
          vector_results: &[VectorSearchResult],
          bm25_results: &[(ChunkId, f32, CodeChunk)],
          k: f32,
      ) -> Vec<SearchResult> {
          let mut scores: HashMap<ChunkId, RrfScore> = HashMap::new();

          // Process vector search results
          for (rank, result) in vector_results.iter().enumerate() {
              let rrf_score = 1.0 / (k + (rank + 1) as f32);
              let entry = scores.entry(result.chunk_id).or_insert_with(...);

              entry.rrf_score += rrf_score * self.config.vector_weight;
              entry.vector_score = Some(result.score);
              entry.vector_rank = Some(rank + 1);
          }

          // Process BM25 results
          for (rank, (chunk_id, score, chunk)) in bm25_results.iter().enumerate() {
              let rrf_score = 1.0 / (k + (rank + 1) as f32);
              let entry = scores.entry(*chunk_id).or_insert_with(...);

              entry.rrf_score += rrf_score * self.config.bm25_weight;
              entry.bm25_score = Some(*score);
              entry.bm25_rank = Some(rank + 1);
          }

          // Convert to SearchResult and sort by RRF score
          let mut results: Vec<SearchResult> = scores
              .into_values()
              .map(|rrf_score| SearchResult { ... })
              .collect();

          results.sort_by(|a, b| {
              b.score.partial_cmp(&a.score).unwrap_or(std::cmp::Ordering::Equal)
          });

          results
      }

  rrf_score_struct:
    location: "src/search/mod.rs:356-365"
    fields:
      chunk_id: "ChunkId - Unique identifier"
      rrf_score: "f32 - Combined RRF score"
      vector_score: "Option<f32> - Original vector similarity"
      vector_rank: "Option<usize> - Rank in vector results"
      bm25_score: "Option<f32> - Original BM25 score"
      bm25_rank: "Option<usize> - Rank in BM25 results"
      chunk: "CodeChunk - The actual code chunk"

  key_properties:
    benefits:
      - "Score normalization - No need to normalize BM25 and vector scores"
      - "Robust to outliers - Uses ranks instead of raw scores"
      - "Simple implementation - Easy to understand and maintain"
      - "Proven effectiveness - Used in production IR systems"

    parameters:
      k_parameter:
        default: 60.0
        range: "10.0 to 100.0"
        effect: "Higher k = less emphasis on top-ranked items"
        location: "src/search/mod.rs:27"

      weights:
        bm25_weight:
          default: 0.5
          range: "0.0 to 1.0"
          location: "src/search/mod.rs:23"
        vector_weight:
          default: 0.5
          range: "0.0 to 1.0"
          location: "src/search/mod.rs:25"

  example_calculation:
    scenario: "Chunk appears at rank 1 in vector, rank 2 in BM25"
    k: 60.0
    weights:
      vector: 0.5
      bm25: 0.5
    calculation: |
      vector_contribution = 0.5 * (1 / (60 + 1)) = 0.5 * 0.01639 = 0.008197
      bm25_contribution = 0.5 * (1 / (60 + 2)) = 0.5 * 0.01613 = 0.008065
      total_rrf_score = 0.008197 + 0.008065 = 0.016262

# ==============================================================================
# HYBRID SEARCH FLOW
# ==============================================================================
hybrid_search_flow:
  overview: |
    The hybrid search combines BM25 and vector search in parallel, then
    merges results using RRF. The system handles failures gracefully with
    automatic fallback mechanisms.

  main_search_function:
    location: "src/search/mod.rs:129-180"
    steps:
      - step: 1
        name: "Parallel Search Execution"
        location: "src/search/mod.rs:147-173"
        code: |
          let (vector_results, bm25_results) = if let Some(bm25) = &self.bm25_search {
              let bm25_clone = bm25.clone();
              let query_clone = query.to_string();
              let candidate_count = self.config.candidate_count;

              let (vector_future, bm25_future) = tokio::join!(
                  self.vector_search.search(query, candidate_count),
                  tokio::task::spawn_blocking(move || {
                      bm25_clone.search(&query_clone, candidate_count)
                  })
              );

              let vector_results = vector_future?;
              let bm25_results = bm25_future??;

              (vector_results, bm25_results)
          } else {
              // Vector-only fallback
              let vector_results = self.vector_search.search(query, candidate_count).await?;
              (vector_results, vec![])
          };

      - step: 2
        name: "Apply RRF Fusion"
        location: "src/search/mod.rs:176"
        code: |
          let merged = self.reciprocal_rank_fusion_with_k(&vector_results, &bm25_results, rrf_k);

      - step: 3
        name: "Return Top N Results"
        location: "src/search/mod.rs:179"
        code: |
          Ok(merged.into_iter().take(limit).collect())

  parallel_execution:
    description: |
      Vector search (async) and BM25 search (sync) run in parallel using tokio::join!.
      BM25 runs in a blocking task to avoid blocking the async runtime.
    location: "src/search/mod.rs:153-158"
    benefits:
      - "Reduced latency - Both searches run simultaneously"
      - "Efficient resource usage - No waiting between searches"
      - "Proper async/sync integration - spawn_blocking for CPU-bound BM25"

  candidate_count:
    purpose: "Fetch more candidates than final limit for better RRF quality"
    default: 100
    location: "src/search/mod.rs:29"
    rationale: |
      By fetching 100 candidates from each engine and only returning 10 final results,
      RRF has more options to find the best combination of BM25 and vector matches.

# ==============================================================================
# RESULT MERGING STRATEGY
# ==============================================================================
result_merging:
  strategy: "Reciprocal Rank Fusion (RRF)"

  merge_process:
    step_1_initialize:
      description: "Create HashMap to accumulate scores by ChunkId"
      location: "src/search/mod.rs:203"
      code: "let mut scores: HashMap<ChunkId, RrfScore> = HashMap::new();"

    step_2_process_vector:
      description: "Add RRF scores from vector search rankings"
      location: "src/search/mod.rs:206-221"
      logic: |
        For each vector result at rank i:
          1. Calculate RRF contribution: vector_weight / (k + i)
          2. Add to HashMap, creating entry if needed
          3. Store original vector score and rank

    step_3_process_bm25:
      description: "Add RRF scores from BM25 rankings"
      location: "src/search/mod.rs:224-239"
      logic: |
        For each BM25 result at rank i:
          1. Calculate RRF contribution: bm25_weight / (k + i)
          2. Add to existing entry or create new one
          3. Store original BM25 score and rank

    step_4_convert_and_sort:
      description: "Convert to SearchResult and sort by combined score"
      location: "src/search/mod.rs:242-262"
      code: |
        let mut results: Vec<SearchResult> = scores
            .into_values()
            .map(|rrf_score| SearchResult {
                chunk_id: rrf_score.chunk_id,
                score: rrf_score.rrf_score,
                bm25_score: rrf_score.bm25_score,
                vector_score: rrf_score.vector_score,
                bm25_rank: rrf_score.bm25_rank,
                vector_rank: rrf_score.vector_rank,
                chunk: rrf_score.chunk,
            })
            .collect();

        results.sort_by(|a, b| {
            b.score.partial_cmp(&a.score).unwrap_or(std::cmp::Ordering::Equal)
        });

  deduplication:
    method: "Automatic via HashMap keyed by ChunkId"
    location: "src/search/mod.rs:203"
    behavior: |
      When a chunk appears in both BM25 and vector results:
      1. Single entry is created in HashMap
      2. Both RRF contributions are added
      3. Both original scores and ranks are preserved
      4. Result has higher combined score than single-source matches

  search_result_structure:
    location: "src/search/mod.rs:43-60"
    fields:
      - name: "chunk_id"
        type: "ChunkId"
        description: "Unique chunk identifier"

      - name: "score"
        type: "f32"
        description: "Combined RRF score"

      - name: "bm25_score"
        type: "Option<f32>"
        description: "Original BM25 score if found in BM25 results"

      - name: "vector_score"
        type: "Option<f32>"
        description: "Original vector similarity if found in vector results"

      - name: "bm25_rank"
        type: "Option<usize>"
        description: "Rank position in BM25 results (1-indexed)"

      - name: "vector_rank"
        type: "Option<usize>"
        description: "Rank position in vector results (1-indexed)"

      - name: "chunk"
        type: "CodeChunk"
        description: "The actual code chunk with context"

# ==============================================================================
# SCORE NORMALIZATION
# ==============================================================================
score_normalization:
  approach: "Rank-based (no explicit normalization needed)"

  rationale:
    description: |
      RRF uses ranks instead of raw scores, eliminating the need for explicit
      score normalization. This is one of RRF's key advantages.

  why_no_normalization:
    - "BM25 scores are unbounded (can be any positive value)"
    - "Vector scores are bounded [0, 1] (cosine similarity)"
    - "Directly combining these would favor one system"
    - "RRF converts to ranks: 1st, 2nd, 3rd... (same scale for both)"
    - "Then applies formula: 1/(k + rank) - normalized by design"

  implicit_normalization:
    formula: "1 / (k + rank)"
    bounds: "(0, 1/k)"
    example_with_k60:
      rank_1: "1 / (60 + 1) = 0.01639"
      rank_2: "1 / (60 + 2) = 0.01613"
      rank_10: "1 / (60 + 10) = 0.01429"
      rank_100: "1 / (60 + 100) = 0.00625"

  weight_application:
    location: "src/search/mod.rs:218,236"
    bm25: "entry.rrf_score += rrf_score * self.config.bm25_weight;"
    vector: "entry.rrf_score += rrf_score * self.config.vector_weight;"
    effect: |
      Weights control relative importance:
      - bm25_weight = 0.7, vector_weight = 0.3 → favor lexical matching
      - bm25_weight = 0.3, vector_weight = 0.7 → favor semantic similarity
      - bm25_weight = 0.5, vector_weight = 0.5 → equal importance (default)

# ==============================================================================
# CONFIGURATION PARAMETERS
# ==============================================================================
configuration:
  hybrid_search_config:
    location: "src/search/mod.rs:19-41"
    struct: |
      pub struct HybridSearchConfig {
          pub bm25_weight: f32,      // Default: 0.5
          pub vector_weight: f32,    // Default: 0.5
          pub rrf_k: f32,           // Default: 60.0
          pub candidate_count: usize // Default: 100
      }

  parameters:
    bm25_weight:
      type: "f32"
      range: "0.0 to 1.0"
      default: 0.5
      location: "src/search/mod.rs:23"
      effect: "Higher = more importance to lexical matching"

    vector_weight:
      type: "f32"
      range: "0.0 to 1.0"
      default: 0.5
      location: "src/search/mod.rs:25"
      effect: "Higher = more importance to semantic similarity"

    rrf_k:
      type: "f32"
      range: "10.0 to 100.0 (recommended)"
      default: 60.0
      location: "src/search/mod.rs:27"
      effect: |
        - Lower k (e.g., 10): More emphasis on top-ranked items
        - Higher k (e.g., 100): More uniform weighting across ranks
        - Typical range: 40-80

    candidate_count:
      type: "usize"
      default: 100
      location: "src/search/mod.rs:29"
      effect: |
        Number of candidates fetched from each search engine before RRF.
        Higher values give RRF more options but increase latency.

  tuning_support:
    automatic_tuning:
      file: "src/search/rrf_tuner.rs"
      description: |
        Automatic parameter tuning using test queries and NDCG evaluation.

    rrf_tuner:
      location: "src/search/rrf_tuner.rs:19-213"
      features:
        - "Test multiple k values: [10, 20, 40, 60, 80, 100]"
        - "Evaluate using NDCG@10 metric"
        - "Per-query breakdown with verbose mode"
        - "Default Rust code search queries provided"

    usage_example:
      location: "src/search/rrf_tuner.rs:117-157"
      code: |
        let tuner = RRFTuner::default_rust_queries();
        let result = tuner.tune_k(&hybrid_search).await?;

        println!("Best k: {}", result.best_k);
        println!("Best NDCG: {:.4}", result.best_ndcg);

# ==============================================================================
# EVALUATION METRICS
# ==============================================================================
evaluation_metrics:
  overview: |
    The system includes comprehensive evaluation metrics to measure search quality
    and tune parameters automatically.

  metrics_implemented:
    ndcg_at_k:
      name: "Normalized Discounted Cumulative Gain"
      location: "src/search/rrf_tuner.rs:219-239"
      formula: "NDCG = DCG / IDCG"
      dcg_formula: "sum(1 / log2(i + 2)) for relevant items at position i"
      range: "0.0 to 1.0"
      interpretation: "1.0 = perfect ranking, higher is better"
      use_case: "Primary metric for RRF tuning"

    mrr:
      name: "Mean Reciprocal Rank"
      location: "src/search/rrf_tuner.rs:245-251"
      formula: "1 / rank_of_first_relevant_item"
      range: "0.0 to 1.0"
      interpretation: "Higher = first relevant result appears earlier"
      use_case: "Measures how quickly users find relevant results"

    map:
      name: "Mean Average Precision"
      location: "src/search/rrf_tuner.rs:253-271"
      formula: "Average of precision values at each relevant result position"
      range: "0.0 to 1.0"
      interpretation: "Higher = better precision across all ranks"
      use_case: "Overall ranking quality"

    recall_at_k:
      name: "Recall at k"
      location: "src/search/rrf_tuner.rs:273-286"
      formula: "relevant_found_in_top_k / total_relevant"
      range: "0.0 to 1.0"
      interpretation: "Fraction of relevant items retrieved"
      use_case: "Coverage of relevant results"

    precision_at_k:
      name: "Precision at k"
      location: "src/search/rrf_tuner.rs:288-297"
      formula: "relevant_found_in_top_k / k"
      range: "0.0 to 1.0"
      interpretation: "Fraction of retrieved items that are relevant"
      use_case: "Accuracy of top results"

  evaluation_metrics_struct:
    location: "src/search/rrf_tuner.rs:300-307"
    fields:
      - "ndcg_at_10: f64"
      - "mrr: f64"
      - "map: f64"
      - "recall_at_20: f64"
      - "precision_at_10: f64"

  evaluation_function:
    location: "src/search/rrf_tuner.rs:309-340"
    usage: |
      let metrics = evaluate_hybrid_search(&hybrid_search, &test_queries).await?;
      println!("NDCG@10: {:.4}", metrics.ndcg_at_10);
      println!("MRR: {:.4}", metrics.mrr);
      println!("MAP: {:.4}", metrics.map);

# ==============================================================================
# RESILIENT SEARCH & FALLBACK
# ==============================================================================
resilient_search:
  file: "src/search/resilient.rs"
  purpose: "Automatic fallback when search components fail"

  resilient_hybrid_search:
    location: "src/search/resilient.rs:18-206"
    features:
      - "Graceful degradation when components fail"
      - "Automatic fallback to available search method"
      - "Fallback mode tracking for monitoring"

  fallback_hierarchy:
    level_1:
      name: "Full Hybrid Search"
      components: "BM25 + Vector"
      condition: "Both components healthy"

    level_2:
      name: "BM25-only"
      components: "BM25"
      condition: "Vector search fails"
      fallback: "Use only lexical matching"

    level_3:
      name: "Vector-only"
      components: "Vector"
      condition: "BM25 fails"
      fallback: "Use only semantic search"

    level_4:
      name: "Error"
      components: "None"
      condition: "Both fail"
      action: "Return error with details"

  search_method:
    location: "src/search/resilient.rs:54-68"
    code: |
      pub async fn search(&self, query: &str, limit: usize) -> Result<Vec<SearchResult>> {
          // Try full hybrid search first
          match self.try_hybrid_search(query, limit).await {
              Ok(results) => {
                  self.fallback_mode.store(false, Ordering::Relaxed);
                  Ok(results)
              }
              Err(e) => {
                  tracing::warn!("Hybrid search failed: {}, attempting fallback", e);
                  self.fallback_mode.store(true, Ordering::Relaxed);
                  self.fallback_search(query, limit).await
              }
          }
      }

  parallel_component_testing:
    location: "src/search/resilient.rs:76-106"
    code: |
      async fn try_hybrid_search(&self, query: &str, limit: usize) -> Result<Vec<SearchResult>> {
          let (bm25_result, vector_result) = tokio::join!(
              self.bm25_search(query, limit),
              self.vector_search(query, limit)
          );

          match (bm25_result, vector_result) {
              (Ok(bm25), Ok(vector)) => Ok(self.merge_results(bm25, vector)),
              (Ok(bm25), Err(e)) => {
                  tracing::warn!("Vector failed: {}, using BM25 only", e);
                  Ok(bm25)
              }
              (Err(e), Ok(vector)) => {
                  tracing::warn!("BM25 failed: {}, using vector only", e);
                  Ok(vector)
              }
              (Err(bm25_e), Err(vec_e)) => {
                  Err(anyhow!("Both failed - BM25: {}, Vector: {}", bm25_e, vec_e))
              }
          }
      }

# ==============================================================================
# SEARCH RESULT RANKING
# ==============================================================================
result_ranking:
  primary_sort:
    method: "RRF combined score (descending)"
    location: "src/search/mod.rs:256-260"
    code: |
      results.sort_by(|a, b| {
          b.score.partial_cmp(&a.score).unwrap_or(std::cmp::Ordering::Equal)
      });

  score_composition:
    description: |
      Each result's score is the sum of weighted RRF contributions from
      BM25 and vector search engines.

    formula: |
      score = (bm25_weight / (k + bm25_rank)) + (vector_weight / (k + vector_rank))

    ranking_factors:
      - factor: "Rank in BM25 results"
        impact: "Lower rank (higher position) = higher contribution"

      - factor: "Rank in vector results"
        impact: "Lower rank (higher position) = higher contribution"

      - factor: "BM25 weight"
        impact: "Higher weight = more importance to lexical matches"

      - factor: "Vector weight"
        impact: "Higher weight = more importance to semantic matches"

      - factor: "RRF k parameter"
        impact: "Higher k = less emphasis on top positions"

      - factor: "Presence in both results"
        impact: "Chunks in both get boosted (sum of contributions)"

  ranking_scenarios:
    scenario_1:
      description: "Chunk only in vector results at rank 1"
      calculation: |
        score = 0 + (0.5 / (60 + 1)) = 0.00820

    scenario_2:
      description: "Chunk only in BM25 results at rank 1"
      calculation: |
        score = (0.5 / (60 + 1)) + 0 = 0.00820

    scenario_3:
      description: "Chunk in both at rank 1"
      calculation: |
        score = (0.5 / (60 + 1)) + (0.5 / (60 + 1)) = 0.01639
        note: "2x higher than single-source matches"

    scenario_4:
      description: "Chunk in vector rank 1, BM25 rank 5"
      calculation: |
        score = (0.5 / (60 + 5)) + (0.5 / (60 + 1)) = 0.00769 + 0.00820 = 0.01589

# ==============================================================================
# INTEGRATION POINTS
# ==============================================================================
integration_points:
  mcp_tool_interface:
    file: "src/tools/search_tool.rs"
    search_function: "search_tool.rs:236-369"
    workflow:
      step_1:
        action: "Initialize UnifiedIndexer"
        location: "src/tools/search_tool.rs:274-282"

      step_2:
        action: "Index directory (incremental)"
        location: "src/tools/search_tool.rs:286-297"

      step_3:
        action: "Create hybrid search components"
        location: "src/tools/search_tool.rs:315-322"

      step_4:
        action: "Perform hybrid search"
        location: "src/tools/search_tool.rs:325-328"

      step_5:
        action: "Format and return results"
        location: "src/tools/search_tool.rs:331-368"

  unified_indexer:
    file: "src/indexing/unified.rs"
    purpose: "Coordinates indexing to both Tantivy and Qdrant"
    key_methods:
      - "index_directory() - Index entire directory"
      - "create_bm25_search() - Create BM25 search instance"
      - "vector_store_cloned() - Get vector store reference"
      - "embedding_generator_cloned() - Get embedding generator"

  embedding_generation:
    model: "all-MiniLM-L6-v2"
    dimensions: 384
    framework: "fastembed-rs"
    location: "src/embeddings/mod.rs"

# ==============================================================================
# PERFORMANCE CHARACTERISTICS
# ==============================================================================
performance:
  parallel_execution:
    description: "BM25 and vector search run simultaneously"
    implementation: "tokio::join! with spawn_blocking for BM25"
    benefit: "Reduced total search latency (max of both, not sum)"

  candidate_count_impact:
    default: 100
    effect: |
      - Higher count: Better RRF quality but more processing
      - Lower count: Faster but may miss relevant results
      - Recommended: 50-200 depending on use case

  batch_processing:
    upsert: "100 points per batch to Qdrant"
    location: "src/vector_store/mod.rs:218"
    rationale: "Avoid overwhelming server with large uploads"

  caching:
    metadata_cache: "Tracks file hashes for incremental indexing"
    index_reader: "Tantivy reader is reused across searches"
    vector_store: "Qdrant client connection is persistent"

# ==============================================================================
# TESTING & VALIDATION
# ==============================================================================
testing:
  test_files:
    - file: "tests/test_hybrid_search.rs"
      tests:
        - "test_manual_hybrid_search - End-to-end hybrid search"
        - "test_incremental_indexing - Metadata cache validation"
        - "test_qdrant_connection - Vector store connectivity"

    - file: "src/search/mod.rs"
      tests:
        - "test_hybrid_search_config - Default configuration"
        - "test_rrf_calculation - RRF algorithm correctness"
        - "test_vector_only_search - Vector-only fallback"
        - "test_hybrid_search_with_bm25 - BM25 integration"
        - "test_search_result_serialization - Result structure"

    - file: "src/search/bm25.rs"
      tests:
        - "test_bm25_search_creation - Index initialization"
        - "test_bm25_index_and_search - Indexing and querying"
        - "test_bm25_search_multiple_fields - Multi-field search"

    - file: "src/search/rrf_tuner.rs"
      tests:
        - "test_ndcg_perfect_ranking - NDCG calculation"
        - "test_mrr_calculation - MRR metric"
        - "test_recall_at_k - Recall computation"
        - "test_precision_at_k - Precision computation"

  validation_approach:
    component_testing: "Each search engine tested independently"
    integration_testing: "Full hybrid search with real codebase"
    metric_validation: "NDCG and other IR metrics"
    fallback_testing: "Resilient search with component failures"

# ==============================================================================
# KEY DIFFERENCES FROM TYPICAL IMPLEMENTATIONS
# ==============================================================================
unique_features:
  tantivy_over_elasticsearch:
    description: "Uses Tantivy (Rust native) instead of Elasticsearch"
    benefits:
      - "No external service for BM25"
      - "Fast compilation and deployment"
      - "Low resource usage"

  qdrant_integration:
    description: "Uses Qdrant for vector search (not Milvus/Pinecone)"
    benefits:
      - "Open source and self-hostable"
      - "Excellent Rust client"
      - "HNSW graph for fast ANN search"

  resilient_architecture:
    description: "Automatic fallback when components fail"
    benefits:
      - "System remains functional during degradation"
      - "Clear error messages when both fail"
      - "Monitoring through fallback_mode flag"

  comprehensive_metrics:
    description: "Built-in evaluation with NDCG, MRR, MAP, etc."
    benefits:
      - "Quantitative quality measurement"
      - "Automatic parameter tuning"
      - "A/B testing support"

# ==============================================================================
# CONFIGURATION EXAMPLES
# ==============================================================================
configuration_examples:
  balanced_search:
    use_case: "Equal importance to lexical and semantic"
    config:
      bm25_weight: 0.5
      vector_weight: 0.5
      rrf_k: 60.0
      candidate_count: 100

  semantic_focused:
    use_case: "Emphasize semantic similarity over exact matches"
    config:
      bm25_weight: 0.3
      vector_weight: 0.7
      rrf_k: 60.0
      candidate_count: 100

  lexical_focused:
    use_case: "Emphasize keyword matching over semantics"
    config:
      bm25_weight: 0.7
      vector_weight: 0.3
      rrf_k: 60.0
      candidate_count: 100

  aggressive_fusion:
    use_case: "Strongly favor top-ranked items"
    config:
      bm25_weight: 0.5
      vector_weight: 0.5
      rrf_k: 20.0  # Lower k = more emphasis on top ranks
      candidate_count: 50

  conservative_fusion:
    use_case: "More uniform weighting across ranks"
    config:
      bm25_weight: 0.5
      vector_weight: 0.5
      rrf_k: 100.0  # Higher k = less emphasis on top ranks
      candidate_count: 200

# ==============================================================================
# SUMMARY
# ==============================================================================
summary:
  architecture: |
    rust-code-mcp implements a production-ready hybrid search system combining
    Tantivy (BM25) and Qdrant (vector search) with Reciprocal Rank Fusion.

  key_strengths:
    - "Pure Rust implementation with excellent performance"
    - "Resilient architecture with automatic fallback"
    - "Comprehensive evaluation metrics (NDCG, MRR, MAP)"
    - "Automatic parameter tuning support"
    - "Clean separation of concerns (BM25, Vector, RRF, Resilient)"

  search_quality:
    lexical_matching: "BM25 via Tantivy with multi-field search"
    semantic_matching: "Dense embeddings (384D) via Qdrant"
    result_fusion: "RRF with configurable k parameter"
    evaluation: "NDCG@10 as primary quality metric"

  production_readiness:
    error_handling: "Comprehensive with fallback mechanisms"
    monitoring: "Fallback mode tracking, detailed logging"
    configuration: "Tunable parameters with sensible defaults"
    testing: "Unit tests, integration tests, evaluation framework"

  future_enhancements:
    - "Dynamic weight adjustment based on query type"
    - "Query-specific RRF k parameter selection"
    - "Multi-language embedding models"
    - "Cross-encoder reranking for top results"

# ==============================================================================
# CODE SNIPPETS FOR REFERENCE
# ==============================================================================
code_snippets:
  complete_hybrid_search_usage:
    location: "examples/typical_usage.rs (conceptual)"
    code: |
      use file_search_mcp::search::{HybridSearch, HybridSearchConfig};
      use file_search_mcp::embeddings::EmbeddingGenerator;
      use file_search_mcp::vector_store::{VectorStore, VectorStoreConfig};
      use file_search_mcp::search::Bm25Search;

      // Initialize components
      let embedding_gen = EmbeddingGenerator::new()?;
      let vector_store = VectorStore::new(VectorStoreConfig::default()).await?;
      let bm25_search = Bm25Search::new(&index_path)?;

      // Create hybrid search with custom config
      let config = HybridSearchConfig {
          bm25_weight: 0.6,
          vector_weight: 0.4,
          rrf_k: 50.0,
          candidate_count: 80,
      };

      let hybrid_search = HybridSearch::new(
          embedding_gen,
          vector_store,
          Some(bm25_search),
          config,
      );

      // Perform search
      let results = hybrid_search.search("async file processing", 10).await?;

      // Analyze results
      for result in results {
          println!("Score: {:.4}", result.score);
          if let Some(bm25) = result.bm25_score {
              println!("  BM25: {:.2} (rank {})", bm25, result.bm25_rank.unwrap());
          }
          if let Some(vector) = result.vector_score {
              println!("  Vector: {:.4} (rank {})", vector, result.vector_rank.unwrap());
          }
          println!("  {}: {}", result.chunk.context.symbol_name,
                   result.chunk.context.file_path.display());
      }

  rrf_parameter_tuning:
    location: "examples/tune_rrf.rs (conceptual)"
    code: |
      use file_search_mcp::search::rrf_tuner::{RRFTuner, TestQuery};

      // Create tuner with test queries
      let test_queries = vec![
          TestQuery {
              query: "parse JSON data".to_string(),
              relevant_chunk_ids: vec!["parse_json".to_string(), "JsonParser".to_string()],
          },
          // ... more queries
      ];

      let tuner = RRFTuner::new(test_queries);

      // Tune k parameter
      let result = tuner.tune_k(&hybrid_search).await?;

      println!("Optimal k: {}", result.best_k);
      println!("Best NDCG@10: {:.4}", result.best_ndcg);

      // Apply optimal k to search config
      let optimal_config = HybridSearchConfig {
          rrf_k: result.best_k,
          ..Default::default()
      };

  resilient_search_usage:
    location: "examples/resilient_search.rs (conceptual)"
    code: |
      use file_search_mcp::search::ResilientHybridSearch;

      // Create resilient search (handles component failures)
      let resilient = ResilientHybridSearch::new(
          Some(bm25_search),
          Some(vector_store),
          Some(embedding_gen),
          60.0,  // rrf_k
      );

      // Search - automatically falls back if needed
      let results = resilient.search("error handling", 10).await?;

      // Check if fallback mode was triggered
      if resilient.is_fallback_mode() {
          println!("Warning: Search using fallback mode");
      }

---
